# 单处理器计算（一）

> 翻自：《Introduction to High Performance Scientific Computing》-Victor Eijkhout

为了编写高效的科学代码，了解计算机体系结构是很重要的。计算相同结果的两个代码之间的速度差异可能从几个百分点到数量级不等，这只取决于与处理器架构的算法编码情况有关的因素。显然，仅仅有一个算法并“把它放到计算机上”是不够的：掌握一些计算机架构的知识是明智的，有时是至关重要的。

有些问题可以在单个中央处理单元(CPU)上解决，有些问题需要由多个处理器组成的并行计算机。我们将在下一章详细介绍并行计算机，但即使是并行处理，也有必要了解单个CPU。

在这一章中，我们将重点关注CPU及其内存系统内部发生的事情。我们首先简单讨论指令是如何处理的，然后我们将研究处理器核心中的算术处理；最后但并非最不重要的是，我们将重点关注内存和处理器之间以及处理器内部的数据移动。可能出乎意料的是，后一点非常重要，因为内存访问通常比执行处理器指令慢得多，这使得它成为程序性能的阻碍因素；“flops(每秒浮点操作数)计数”是预测代码性能的关键的时代已经一去不复返了。这种差异实际上是一个不断增长的趋势，所以随着时间的推移，处理内存流量的问题变得越来越重要，而不是消失。

这一章将让你对CPU设计中涉及的问题有一个基本的了解，它是如何影响性能的，以及如何编写优化性能的代码。更多细节，请参阅关于PC架构的在线书籍[114]，以及关于计算机架构的标准工作，Hennesey和Patterson[97]。

## 冯诺依曼架构

虽然计算机，以及与本章最相关的，它们的处理器可以在许多细节上有所不同，但它们也有许多共同之处。在一个非常高的抽象层次上，许多架构都可以被描述为冯·诺伊曼架构。它描述了一种具有不可分割内存的设计，存储程序和数据(存储程序)，以及一个执行指令的处理单元，在“取、执行、存储周期”中对数据进行操作。

**注释1： ** 这个具有指定指令序列的模型也称为控制流。这是与数据流相反的，我们将在后面中看到。

这种设置使现代处理器区别于最早的和一些特殊目的的现代处理器，在这些设计中程序是硬连接的。它还允许程序修改自己或生成其他程序，因为指令和数据在同一个存储器中。这使得我们有了编辑器和编译器:计算机将程序代码当作数据来处理。

**注释2：** 曾经，存储程序的概念作为一个基本组件被包括进来，使一个正在运行的程序能够修改它自己的源代码。然而，人们很快意识到这将导致不可维护的代码，并且在实践中很少这样做。

在本书中，我们不会明确讨论编译器，即把高级语言翻译成机器指令的程序。然而，有时我们将讨论如何编写高层次的程序以确保低层次的效率。

然而，在科学计算中，我们通常不太关注程序代码，几乎只关注数据及其在程序执行期间如何移动。在大多数实际应用中，程序和数据似乎是分开存储的。关于指令处理的基本内容可以描述如下。

与用户编写的高级语言不同，处理器执行的机器指令通常指定操作的名称，以及操作数和结果的位置。这些位置不是表示为内存位置，而是表示为寄存器：是CPU的一部分的少量命名内存位置。

**注释3：** 直接到内存的架构是罕见的，尽管它们已经存在。20世纪80年代的Cyber 205超级计算机可以同时有三个数据流，两个从内存到处理器，一个从处理器到内存。这样的架构只有在内存能够跟上处理器速度的情况下才可行，而现在已经不是这样了。

下面是一个简单的C例子：

```c
void store(double *a, double *b, double *c) {
        *c = *a + *b;
}
```

及其X86汇编输出，由`gcc -O2 -S -o - store.c`：

```c
					.text
          .p2align 4,,15
    .globl store
          .type		store, @function
     store:
					 movsd	(%rdi), %xmm0  # Load *a to %xmm0
           addsd	(%rsi), %xmm0  # Load *b and to %xmm0
           movsd	 %xmm0, (%rdx) # Store to *c
           ret
```

(64位系统输出；32位系统可以添加`-m64`指令输出)

使用说明如下：

- 从内存到寄存器的加载；
- 另一个负载，加上一个附加项；
- 把结果写回内存。

每条指令的处理如下：

- 指令获取：根据程序计数器将下一条指令装入进程或程序中。我们将忽略这是如何发生以及从哪里发生的问题。
- 指令解码：处理器检查指令以确定操作和操作数。
- 内存获取：如果需要，数据从内存中取出到寄存器中。
- 执行：执行操作，从寄存器读取数据并将数据写回寄存器。

数组数据的情况稍微复杂一些：然后将加载(或存储)的元素确定为数组的基址加上偏移量。

在某种程度上，现代CPU在程序员看来就像冯·诺伊曼机器。在很多方面，情况并非如此。首先，虽然内存看起来是随机可寻址的，但在实践中存在着局域性的概念:一旦一个数据项被加载，附近的项将更有效地加载，而重新加载初始项也会更快。

简单数据加载的另一个复杂之处是，当前的CPU同时操作多条指令，这些指令被称为“在飞行中”，这意味着它们处于不同的完成阶段。当然，与这些同步指令一起，它们的输入和输出也以重叠的方式在内存和处理器之间移动。这是超标量CPU体系结构的基本思想，也被称为指令级并行(ILP)。因此，虽然每个指令可能需要几个时钟周期才能完成，但处理器可以在有利的情况下每个周期完成一条指令;在某些情况下，每个周期可以完成多条指令。

关于CPU的主要统计数据是它们的千兆赫级，这意味着处理器的速度是计算机性能的主要决定因素。虽然速度显然与性能相关，但实际情况更为复杂。有些算法是受CPU限制的，而进程的速度实际上是最重要的因素；其他算法受到内存的限制，而总线速度和缓存大小等方面(稍后将讨论)变得很重要。

在科学计算中，第二种类型实际上是相当突出的，因此在本章中，我们将大量关注将数据从内存转移到处理器的过程，而对实际处理器的关注相对较少。

## 当代处理器

现代的处理器相当复杂，在这一节中，我们将简短地介绍一下它们的组成部分。下图是Intel Sandy Bridge处理器的芯片图。这种芯片大约一英寸大小，包含近十亿个晶体管。

<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/sandybridge-eightcore-ann.jpg" alt="sandybridge-eightcore-ann" style="zoom:30%;" />



### 处理核心

在冯·诺依曼模型中，只有一个执行指令的实体。自21世纪初以来，这并不是增加措施的情况。上图所示的Sandy Bridge有8个核，每个核都是执行指令流的独立单元。在本章中，我们将主要讨论单个核心的各个方面；第1.4节将讨论多核的集成方面。



#### 指令处理

冯·诺伊曼模型也是不现实的，因为它假设所有的指令都严格按照顺序执行。在过去的二十年中，处理器越来越多地使用了无序指令处理，即指令可以按照不同于用户程序指定的顺序进行处理。当然，处理器只有在不影响执行结果的情况下才允许对指令重新排序！

在框图中，你可以看到与指令处理有关的各种单元:这种聪明的做法实际上要花费相当多的能源，以及大量的晶体管。由于这个原因，处理器，如第一代英特尔Xeon Phi协处理器，*Knights Corner*，使用顺序指令处理。然而，在下一代，*Knights Landing*，这一决定被推翻的rea- sons的表现。



#### 浮点单元

在科学计算中，我们最感兴趣的是处理器如何处理浮点数据。用整数或布尔值进行计算通常不那么有趣。因此，核心在处理数值数据方面具有相当的复杂性。

例如，过去的处理器只有一个浮点单元(FPU)，而现在它们将有多个，能够同时执行。

例如，加法和乘法通常是分开的;如果编译器可以找到独立的加法和乘法操作，它就可以调度它们，使它们同时执行，从而使处理器的性能翻倍。在某些情况下，一个处理器会有多个加法或乘法单元。
另一种提高性能的方法是使用融合倍加(FMA)单元，它可以在与单独的加法或乘法相同的时间内执行指令$x$ $\leftarrow$$ ax$ +$ b$。加上流水线操作(见下文)，这意味着处理器在每个时钟周期中有几个浮点运算的渐近速度。



<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/sandybridge_pipeline.jpg" alt="sandybridge_pipeline" style="zoom:75%;" />

顺便说一句，除法运算是一个限制因子的算法很少。相应地，在现代CPU中，除法操作的优化程度远不及加法和乘法。除法操作可能需要10或20个时钟周期，而CPU可以有多个加法和/或乘法单元(渐进地)，每个周期可以产生一个结果。下表为多个处理器架构的浮点能力(每个核心)，以及8个操作数的DAXPY周期数

| Processor          | year | add/mult/fma units(count×width) | daxpy cycles(arith vs load/store) |      |      |
| :----------------- | :--: | :-----------------------------: | :-------------------------------: | ---- | ---- |
| MIPS R10000        | 1996 |            1×1+1×1+0            |               8/24                |      |      |
| Alpha EV5          | 1996 |            1×1+1×1+0            |               8/12                |      |      |
| IBM Power5         | 2004 |             0+0+2×1             |               4/12                |      |      |
| AMD Bulldozer      | 2011 |            2×2+2×2+0            |                2/4                |      |      |
| Intel Sandy Bridge | 2012 |            1×4+1×4+0            |                2/4                |      |      |
| Intel Haswell      | 2014 |             0+0+2×4             |                1/2                |      |      |

#### 流水线

处理器的浮点加和乘单元是流水线的，其效果是独立的操作流可以以每个时钟周期一个结果的渐近速度执行。流水线背后的想法如下。假设一个操作由多个更简单的操作组成，并且每个子操作在处理器中都有独立的硬件。例如，加法指令可以包含以下组件：

- 解码指令，包括查找操作数的位置。
- 将操作数复制到寄存器(' data fetch ')。
- 调整指数；加上$$.35 ×$$ $10^{-1}$ + $.6$ $×$ $10^{−2}$就变成$.35 ×$ $10^{-1}$ + $.06$  $×$ $10^{−1}$。
- 执行尾数的加法，在这个例子中是.41。
- 将结果归一化，在本例中为$.41 × 10^{−1}$。(本例中的规范化则没有做任何事.$3 × 100 + .8 × 100$和$.35 × 10^{−3} +(−.34)× 10^{−3}$有一个不小的调整。)

这些部分通常被称为流水线的“阶段”或“部分”。

如果每个部件设计为1个时钟周期，则整个指令需要6个周期。然而，如果每个操作都有自己的硬件，我们可以在少于12个周期内执行两个操作：

- 执行第一个操作的解码阶段；
- 为第一个操作取数据，同时为第二个操作解码。
- 同时执行第一操作的第三阶段和第二操作的第二阶段。
- 以此类推。

你可以看到，第一个操作仍然需要6个时钟周期，但第二个操作只需晚1个周期就完成了。

让我们对流水线获得的加速做一个正式的分析。在传统的FPU上，产生n个结果需要花费$t(n)=nlt$，其中$l$是状态个数，而$t$是时钟周期。结果产生的速率是$t(n)/n$的倒数：$r_{serial}\equiv(lt)^{-1}$

另一方面，对于流水线的FPU，时间是$t(n)=[s+l+n-1]t$其中$s$是设置成本；第一次操作仍然必须经历与之前相同的阶段，但在那之后每个周期将产生更多的结果。我们可以记作公式为$t(n)=[n+n_{1/2}]t $表示线性时间，加上偏移量。流水线操作的示意图描述如下：

<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/pipeline.jpg" alt="pipeline" style="zoom: 33%;" />

**练习1.1** 让我们比较一下传统FPU和流水线FPU的速度。证明结果速率现在依赖于$n$：给出$r(n)$和$r_\infty = \lim\limits_{n\rightarrow \infty}r(n)$的公式。在非流水线情况下$r$的加速极限是什么样？它需要多长时间才能接近极限情况？注意到$n=n_{1/2}$你可以得到$r(n)=r_\infty/2$，这通常被用做$n_{1/2}$的定义。

由于向量处理器同时处理多个指令，这些指令必须是独立的。操作$∀_i:a_i\leftarrow b_i + c_i$有独立的加法运算；操作$\forall_i:a_{i+1}\leftarrow a_ib_i+c_i$将一次迭代$(a_i)$的结果输入到下一次迭代的输入$(a_{i+1}=...)$，所以这些操作不是独立的。

与以前的CPU相比，流水线处理器可以将操作速度提高4、5、6倍。在上世纪80年代，当第一台成功的矢量计算机上市时，这样的数字是典型的。现在，CPU可以有20个阶段的流水线。这是否意味着它们的速度非常快？这个问题有点复杂。芯片设计者不断提高时钟速率，流水线部分不再能够在一个周期内完成他们的工作，所以他们进一步分裂。有时甚至有一些片段什么也没有发生：这段时间是需要的，以确保数据可以及时传输到芯片的不同部分。

你可以从流水线CPU得到的改进是有限的，所以为了追求更高的性能，我们尝试了几种不同的流水线设计。例如，Cyber 205有单独的加法和乘法流水线，可以将一个流水线输入另一个流水线，而无需先将数据返回内存。像$∀_i: a_i\leftarrow b_i + c·d_i$这样的操作被称为“链接三元组”(因为到内存的路径数量，一个输入操作数必须是标量)。

**练习1.2** 分析关联三元组的加速和$n_{1/2}$。

另一种提高性能的方法是使用多个相同的流水线。NEC SX系列完善了这种设计。例如，有4条流水线时，$∀_i:a_i\leftarrow b_i +c_i$操作将对模块4进行拆分，以便第一条流水线对索引$i = 4·j$操作，第二条流水线对索引$i = 4·j + 1$操作，以此类推。

**练习1.3** 分析具有多个并行操作流水线的处理器的加速和$n_{1/2}$。也就是说，假设有$p$个执行相同指令的独立流水线，每条流水线都可以处理操作数流。

（你可能想知道我们为什么在这里提到一些相当老的计算机：真正的流水线超级计算机已经不存在了。在美国，Cray X1是该系列的最后一款，而在日本，只有NEC还在生产。然而，现在CPU的功能单元是流水线的，所以这个概念仍然很重要。）

**练习1.4** 如下操作

```c
for (i) {
  x[i+1] = a[i]*x[i] + b[i];
}
```

不能由流水线处理，因为在操作的一次迭代的输入和前一次迭代的输出之间存在依赖关系。但是，你可以将循环转换为数学上等价的循环，并且可能更有效地计算。导出一个表达式，该表达式从$x[i]$中计算$x[i+2]$而不涉及$x[i+1]$。这就是所谓的递归加倍。假设你有足够的临时存储空间。你现在可以通过

- 做一些初步的计算；
- 计算$x[i],x[i+2],x[i+4],...$，并从这些中
- 计算缺失项$x[i+1],x[i+3],...$

通过给出$T_0(n)$和$T_s(n)$的计算公式，分析了该格式的有效性。你能想到为什么初步计算在某些情况下可能不那么重要吗？

##### **收缩计算**

上面描述的流水线是收缩算法的一种情况。在20世纪80年代和90年代，有研究使用流水线算法和构建特殊硬件系统阵列来实现它们[125]。这也与现场可编程门阵列(fpga)的计算连接，其中收缩压阵列是由软件定义的。



#### 最佳性能

由于流水线的作用，对于现代CPU来说，时钟速度和峰值性能之间存在着简单的关系。由于每个FPU可以在一个周期内产生一个结果，所以峰值性能是时钟速度乘以独立FPU的数量。浮点运算性能的衡量标准是“每秒浮点运算”，缩写为flops。考虑到现在计算机的速度，你会经常听到浮点运算被表示为“gigaflops”：$10^9$次浮点运算的倍数。



### 8位，16位，32位，64位

处理器的特征通常是它们作为一个单元可以处理多大的数据块。这可以与

- 处理器和内存之间路径的宽度:一个64位的浮点数可以在一个周期内加载，还是以块的形式到达处理器。
- 内存的寻址方式:如果地址被限制为16位，只有64,000字节可以被识别。早期的pc机有一个复杂的方案与段绕过这个限制:一个地址指定与段号和段内的偏移量。
- 寄存器中的位数，特别是用于操作数据地址的整数寄存器的大小；参见前一点。(浮点寄存器通常更大，例如在x86体系结构中是80位。)这也对应于处理器可以同时操作的数据块的大小。
- 浮点数的大小。如果CPU的算术单元被设计成有效地乘8字节数(' double precision '；见3.2.2节)，那么一半大小的数字(“单精度”)有时可以以更高的效率处理，而对于更大的数字(“四倍精度”)，则需要一些复杂的方案。例如，一个四精度数可以用两个指数之间有固定差异的双精度数来模拟。

这些测量值不一定相同。例如，原来的奔腾处理器有64位数据总线，但有一个32位处理器。另一方面，摩托罗拉68000处理器(最初的苹果Macintosh)有一个32位CPU，但16位数据总线。

第一个英特尔微处理器4004是一个4位处理器，它可以处理4位的数据块。如今，64位处理器正在成为标准。



### Caches：芯片上的内存

计算机内存的大部分是在与处理器分离的芯片中。然而，通常有少量的片上内存(通常是几兆字节)，称为高速缓存。这将在第1.3.4节中详细解释。



### 图形、控制器、专用硬件

“消费型”和“服务器型”处理器之间的一个区别是，消费型芯片在处理器芯片上花了相当大的空间用于图形处理。手机和平板电脑的处理器甚至可以有专门的安全电路或mp3播放电路。处理器的其他部分专门用于与内存或I/O子系统通信。我们将不在本书中讨论这些方面。



### 超标量处理和指令级并行性

在冯·诺伊曼模型中，处理器通过控制流进行操作:指令之间线性地或通过分支相互跟踪，而不考虑它们涉及哪些数据。随着处理器变得越来越强大，一次可以执行多条指令，就有必要切换到数据流模型。这种超标量处理器分析多个指令以找到数据相关性，并并行执行彼此不依赖的指令。

这个概念也被称为指令级并行(ILP)，它被各种机制促进：

- 多发：独立指令可同时启动；
- 流水线：上文提到，算术单元可以在不同的完成阶段处理多个操作；
- 分支预测和推测执行:编译器可以“猜测”一条条件指令的值是否为真，然后相应地执行这些指令；
- 无序执行：如果指令之间不相互依赖，并且执行效率更高，则指令可以重新排列；
- 预取：数据可以在实际遇到任何需要它的指令之前被推测地请求(这将在后面进一步讨论)。

在上面，我们看到了浮点操作上下文中的流水线操作。现在，整个CPU都是流水线的。不仅是浮点运算，任何类型的指令都将尽快被放入指令流水线中。注意，这个流水线不再局限于相同的指令：流水线的无连接现在被推广到任何同时“在运行中”的部分执行的指令流。

随着时钟频率的增加，处理器流水线的长度也在增加，以使分段在更短的时间内可执行。你已经看到，更长的管道有更大的𝑛1/2，因此需要更多的独立指令来使流水线以充分的效率运行。当达到指令级并行性的极限时，使流水线变长(有时称为“更深”)不再有好处。这通常被认为是芯片设计者转向多核架构的原因，以更有效地使用芯片上的晶体管。

这些较长的管道还有第二个问题：如果代码到达一个分支点(一个条件或循环中的测试)，就不清楚要执行的下一条指令是什么。在那一点上，流水线可能会停止。例如，CPU总是假设测试结果是正确的，因此采取了投机性执行。如果代码随后接受了另一个分支(这称为分支错误预测)，则必须刷新流水线并重新启动。执行流中产生的延迟称为分支惩罚。

## 内存层次结构

现在，我们将完善冯·诺伊曼体系结构的图像，在该体系结构中，数据立即从内存加载到处理器，在处理器中对数据进行操作。这幅图是不现实的，因为所谓的内存墙[204]：内存太慢，无法以处理器能够吸收的速度将数据加载到进程中。具体来说，单个负载可能需要1000个周期，而处理器每个周期可以执行几个操作。(在长时间等待加载之后，下一个加载可能会更快，但对处理器来说仍然太慢。
等待时间与吞吐量的关系将在下面讨论。)

实际上，在FPU和主存储器之间会有不同的存储器级别：寄存器和缓存，一起称为存储器层次结构。它们试图通过使最近使用的数据比从主存中更快地可用来缓解内存墙问题。当然，这是以算法及其实现允许数据被多次使用为前提的。这类数据重用问题将在第1.6.1节中进行更详细的讨论。

寄存器和缓存都在不同程度上比主存快；不幸的是，在某种程度上，内存越快，它就会越小。这些在大小和访问速度上的差异导致了有趣的编程问题，我们将在本章的后面，特别是1.7节讨论这些问题。

我们现在将讨论内存层次结构的各种组件和分析它们行为所需的理论概念。



### Busses

在计算机中移动数据的电线，从存储器到中央处理器或磁盘控制器或屏幕，被称为**总线**。对我们来说最重要的是连接处理器和内存的前端总线(FSB)。在一个流行的架构中，这被称为“北桥”，与连接外部设备(除了图形控制器)的“南桥”相对。

<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/bridges.jpg" alt="bridges" style="zoom: 25%;" />

总线通常比处理器慢，时钟频率略高于1GHz，这是CPU时钟频率的一部分。这是需要缓存的原因之一；事实上，一个处理器可以在每个时钟周期中消耗许多数据项。除了频率，总线的带宽也由每个时钟周期可以移动的比特数决定。在当前的体系结构中，这通常是64或128。现在我们将更详细地讨论这个问题。



### 延迟和带宽

在上面，我们提到了在寄存器中访问数据几乎是瞬时的，而将数据从内存加载到寄存器是任何操作之前的一个必要步骤，会导致大量的延迟。现在我们要让这个故事更精确一点。

有两个重要的概念来描述数据的移动：延迟和带宽。这里的假设是，请求一个数据项会引起初始延迟；如果这一项是数据流的第一个项，通常是连续的内存地址范围，流的其余部分将以一个固定的时间周期不再延迟到达。

**延迟：**是处理器发出内存项请求到内存项实际到达之间的延迟。我们可以区分不同的延迟，比如从内存到缓存的传输，缓存到寄存器的传输，或者将它们总结为内存和处理器之间的延迟。延迟是以(纳米)秒或时钟周期来衡量的。

如果处理器按照在汇编代码中找到的顺序执行指令，则在从内存中提取数据时，执行经常会停止；这也被称为**内存延迟**。因此，低延迟非常重要。在实践中，许多处理器都有指令的“无序执行”，允许它们在等待所请求的数据时执行其他操作。程序员可以考虑这一点，并以一种实现延迟隐藏的方式编写代码；请参见第1.6.1节。图形处理单元(GPU)(见2.9.3节)可以在线程之间快速切换，以实现延迟隐藏。

**带宽：**是克服初始延迟后，数据到达目的地的速率。带宽以字节(千字节、兆字节、千兆字节)/秒或每个时钟周期来衡量。两个存储层之间的带宽通常是通道的周期速度(总线速度)和总线宽度的乘积：总线时钟的每个周期中可以同时发送的比特数。

延迟和带宽的概念通常结合在一个公式中，表示消息从开始到结束所花费的时间：$T(n)=\alpha+\beta n$

其中$\alpha$是延迟，$\beta$是带宽的倒数：即每字节所用的时间。

通常，距离处理器越远，延迟就越长，带宽就越低。这两个因素使得以这样一种方式编程很重要，如果可能的话，处理器使用缓存或寄存器中的数据，而不是主存中的数据。为了说明这是一个严重的问题，考虑向量加法

```c
for(i)
  a[i] = b[i] + c[i]
```

每次迭代执行一个浮点操作，现代cpu可以通过管道在一个时钟周期内完成这一操作。但是，每次迭代都需要加载两个数字，写入一个数字，总共需要24字节的内存流量。(实际上，a[i]在写入之前就被加载了，所以每次迭代有4次内存访问，总共32字节。)典型的内存带宽数字(参见图1.5)远远没有接近24(或32)字节每周期。这意味着，在没有缓存的情况下，算法性能可能受到内存性能的限制。当然，缓存不会加速每一个操作，实际上对上面的示例没有影响。导致大量缓存使用的编程策略将在第1.7节中讨论。

当我们讨论从一个处理器向另一个处理器发送数据时，延迟和带宽的概念也会出现在并行计算机中。



### 寄存器

每个处理器都有少量处理器内部的内存：寄存器或寄存器文件。寄存器是处理器实际操作的对象：例如

```c
a := b + c
```

实际实现为

- 将b的值从内存中装入寄存器，
- 将c的值从内存中装入另一个寄存器，
- 计算和并将其写入另一个寄存器，然后
- 将和值写回a的内存位置。

查看汇编代码(例如编译器的输出)，可以看到显式的加载、计算和存储指令。

像加或乘这样的计算指令只能在寄存器上操作。例如，在汇编语言中，你会看到如下指令

```c
addl	%eax	%edx
```

它将一个寄存器的内容添加到另一个寄存器。正如在这个示例指令中看到的，与内存地址相反，寄存器没有编号，而是具有在汇编指令中引用的不同名称。通常，一个处理器有16或32个浮点寄存器；英特尔安腾(Intel Itanium)的128个浮点寄存器是例外。

寄存器具有高带宽和低延迟，因为它们是处理器的一部分。可以将进出寄存器的数据移动看作是瞬时的。

在本章中，你会发现从内存中移动数据是相对昂贵的。因此，尽可能将数据留在寄存器中是一种简单的优化。例如，如果上面的计算后面跟着一个语句

```c
a := b + c
d := a + e
```

a的计算值可以留在寄存器中。这种优化通常作为编译器优化执行：编译器不会生成存储和重新加载a的指令。我们说a停留在寄存器中。

将值保存在寄存器中通常是为了避免重新计算一个量。例如，在

```c
t1 = sin(alpha) * x + cos(alpha) * y;
t2 = -cos(alpha) * x + sin(alpha) * y;
```

正弦和余弦值可能会保留在寄存器中。你可以通过显式地引入临时数量来帮助编译器：

```c
s = sin(alpha); c = cos(alpha);
t1 = s * x + c * y;
t2 = -c * x + s * y
```

当然，寄存器的数量是有限制的；试图在寄存器中保留太多的权限被称为**寄存器溢出**，这会降低代码的性能。

如果变量出现在内部循环中，那么将该变量保存在寄存器中尤为重要。在计算

```c
for i = 1, length
  a[i] = b[i] * c
```

数量c可能会被编译器保存在寄存器中，而在

```c
for k =1, nevctors
  for i =1, length
    a[i,k] = b[i,k] * c[k]
```

最好是显式地引入一个临时变量来保存c[k]。在C语言中，你可以通过将变量声明为寄存器变量来提示编译器将变量保存在寄存器中：

```c
register double t;
```

### Caches

在寄存器之间,包含指令，直接输入和输出数据和主存大量的数据可以驻留在很长一段时间内，各种级别的缓存内存，有更低的延迟和更高的带宽比主存储器和数据保存为一个中间的时间。

数据从存储器通过高速缓存到达寄存器。使用缓存内存的好处是，如果一个数据项在第一次需要之后不久被重用，那么它仍然在缓存中，因此访问它的速度比从内存中引入数据要快得多。

从历史的角度来看，在1946年的[25]，由于当时内存技术的缓慢，内存层次的概念已经被讨论过了。



#### 一个例子

例如，让我们假设一个变量x被使用了两次，它的使用间隔太大，以至于它会留在寄存器中：

```c
... = ... x ..... //instruction using x
.........					//several instructions not involving x
... = ... x ..... //instruction using x
```

汇编代码为：

- 将x从内存加载到寄存器中，并对其进行操作；
- 执行中间的指令；
- 将x从内存加载到寄存器中，并对其进行操作；

使用缓存，汇编代码保持不变，但内存系统的实际行为现在变成：

- 将x从内存加载到缓存中，并从缓存加载到寄存器中；操作；
- 执行中间的指令；
- 从内存中请求x，但由于它仍然在缓存中，因此将它从缓存加载到寄存器中；执行操作。

由于从缓存加载比从主存加载要快，因此计算速度将会更快。缓存是相当小的，所以值不能无限期地保存在那里。我们将在下面的讨论中看到它的含义。

缓存内存和寄存器之间有一个重要的区别：虽然数据是通过显式的汇编指令移入寄存器的，但从主存到缓存的移动完全是由硬件完成的。因此，缓存的使用和重用不在程序员的直接控制范围之内。稍后，特别是在1.6.2和1.7节中，您将看到如何间接影响缓存的使用。



#### Cache tags

在上面的例子中，没有指定在缓存中找到某个项的机制。为此，每个缓存位置都有一个标记：有足够的信息来重建缓存项来自的内存位置。



#### 缓存级别、速度和大小

这些缓存被称为“level 1”和“level 2”(简称L1和L2)缓存；有些处理器可以有L3缓存。L1和L2缓存是处理器芯片的一部分，尽管L2缓存是最近才发展起来的；L3缓存是芯片外的。L1缓存很小，通常在16Kbyte左右。第2级(如果有，则是第3级)缓存更丰富，最多可达几兆字节，但也比较慢。与可扩展的主存不同，缓存的大小是固定的。如果一个版本的处理器芯片有一个更大的缓存，它通常相当昂贵。

某些操作所需的数据在传送到处理器的过程中被复制到不同的缓存中。如果在一些指令之后，又需要一个数据项，首先在L1缓存中搜索它;如果没有找到，就在L2缓存中搜索；如果没有找到，就从主存中加载。在缓存中找到数据称为缓存命中，没有找到数据称为缓存错过。

图1.5展示了缓存层次结构的基本事实，在这个例子中是针对Intel Sandy Bridge芯片的:缓存越接近fpu，速度越快，但也越小。关于这个数字的几点。

![hierarchysb](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/hierarchysb.jpeg)

- 从寄存器加载数据是如此之快，以致于它不构成算法执行速度的限制。另一方面，注册表很少。每个核心有16个通用寄存器和16个SIMD寄存器。
- L1缓存很小，但是每个周期维持32字节的带宽，即4个双精度数。这足以为两个操作分别加载两个操作数，但请注意，内核实际上每个周期可以执行4个操作。因此，为了达到峰值速度，某些操作数需要留在寄存器中：通常，L1带宽足以满足大约一半的峰值性能。
- L2和L3缓存的带宽名义上与L1相同。然而，这部分带宽浪费在一致性问题上。
- 主内存访问带宽大于100个周期，带宽为4.5字节/周期，约为L1带宽的1/7。然而，这个带宽是由一个处理器芯片的多个核心共享的，因此有效的带宽是这个数字的一个小数。大多数集群每个节点也有多个插槽(处理器芯片)，通常是2或4个，因此一些带宽花费在维持缓存一致性上(参见1.4节)，再次减少了每个芯片的可用带宽。

在第一级，指令和数据有单独的缓存；L2和L3缓存同时包含数据和指令。

可以看到，越来越大的缓存无法足够快地向处理器提供数据。因此，有必要以这样一种方式编码，即数据尽可能地保存在最高缓存级别。我们将在本章的其余部分详细讨论这个问题。

**练习 1.5** L1缓存比L2缓存小，如果有L3缓存，则L2要比L3小。给出一个实际的和理论上的原因。



#### 缓存丢失的类型

有三种类型的缓存丢失。
正如在上面的例子中看到的，第一次引用数据时，总是会导致缓存丢失，这被称为强制性缓存丢失，因为这些是不可避免的。这是否意味着在第一次需要数据项时，我们要一直等待它？不一定：第1.3.5节解释了硬件如何通过预测下一步需要什么数据来帮助您。

下一种类型的缓存丢失是由于工作集的大小造成的：容量缓存丢失是由于数据被覆盖，因为缓存不能包含所有问题数据。(第1.3.4.6节讨论了处理器如何决定要覆盖哪些数据。)如果您想要避免这种类型的失误，您需要将问题划分为足够小的块，以便数据可以在缓存中停留相当长的时间。当然，这是在假设数据项被多次操作的前提下，所以把数据项保存在缓存中是有意义的;这将在第1.6.1节中讨论。

最后，由于一个数据项被映射到与另一个相同的缓存位置而导致的冲突缺失，而这两个数据项仍然是计算所需要的，并且可能有更好的候选者需要被驱逐。这将在1.3.4.10节中讨论。

在多核上下文中还有另外一种类型的缓存丢失：invalidation miss。如果缓存中的某个项因为另一个内核改变了相应内存地址的值而失效，就会发生这种情况。内核将不得不重新加载这个地址。



#### 重复利用是关键

一个或多个缓存的存在并不能立即保证高性能:这在很大程度上取决于代码的内存访问模式，以及如何充分利用缓存。第一次引用一个项时，它被从内存复制到缓存，并通过处理器寄存器。缓存的存在并没有以任何方式减少延迟和带宽。当同一项第二次被引用时，它可能在缓存中被找到，在延迟和带宽方面的成本大大降低:缓存比主存有更短的延迟和更高的带宽。

我们的结论是，首先，算法必须有数据重用的机会。如果每个数据项只被使用一次(就像除了两个向量之外)，那么就不能重用，缓存的存在在很大程度上是无关紧要的。只有当缓存中的项被多次引用时，代码才会从缓存增加的带宽和减少的延迟中受益;详细的讨论请参见1.6.1节。例如，矩阵向量乘法$𝑦=𝐴𝑥$，其中$𝑥$的每个元素都在$𝑛$操作中使用，其中$𝑛$是矩阵维数。

其次，算法理论上可能有重用的机会，但需要以实际公开重用的方式进行编码。我们将在1.6.2节中解决这些问题。这第二点尤其重要。

有些问题很小，可以完全放在缓存中，至少在L3缓存中是这样。这是在进行基准测试时需要注意的一点，因为它对处理器性能的描述过于乐观了。



#### 替换策略

高速缓存和寄存器中的数据由系统放置在那里，而不是由程序员控制。同样地，当缓存或寄存器中的数据在一段时间内没有被引用，并且其他数据需要放在那里时，系统决定什么时候覆盖这些数据。下面,我们将会详细介绍了缓存如何做到这一点,但在整合一个总体原则，一个最近最少使用(LRU)缓存替换策略使用：如果缓存已满,需要放入新数据,最近最少使用的数据从缓存刷新，这意味着它是覆盖在新项目，因此不再访问。LRU是目前最常见的替换政策;其他的可能性是先进先出(FIFO)或随机替换。

**练习 1.6** LRU替换策略与直接映射缓存和关联缓存有什么关系？

**练习 1.7** 描绘一个简单的场景，并给出一些(伪)代码，以论证LRU比FIFO更适合作为替代策略。



#### Cache lines

在内存和高速缓存之间或高速缓存之间的数据移动不是用单个字节，甚至字来完成的。相反，移动数据的最小单位称为高速缓存线，有时也称为高速缓存块。一个典型的缓存行是64或128字节长，在科学计算的上下文中意味着8或16个双精度浮点数。移动到L2缓存的数据的缓存线大小可能比移动到L1缓存的数据大。

高速缓存线的第一个动机是实用的:它简化了所涉及的电路。其次，cachelines是有意义的，因为许多代码显示空间局部性；1.6.2部分。

相反，现在有强烈的动机以这种方式编写代码来利用这个局部性，因为任何内存访问都需要传输几个单词(参见1.7.4节中的一些例子)。一个高效的程序然后尝试使用缓存线上的其他项，因为对它们的访问实际上是自由的。这种现象在通过stride访问数组的代码中是可见的：以规则的时间间隔读取或写入元素。

<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/stride-1.jpg" alt="stride-1" style="zoom:50%;" />

Stride 1 对应于数组的顺序访问：

```c
for (i=0; i<N; i++)
  ... = ... x[i] ...
```

让我们用一个case来说明，每case 4个字。请求第一个元素将包含它的整个cache line加载到缓存中。然后，对第2、3和4个元素的请求可以从缓存中得到满足，这意味着具有高带宽和低延迟。

```c
A larger stride
  for (i=1; i<N; i+=stride)
    ... = ... x[i] ...
```

意味着在每个缓存行中只有某些元素使用。我们用larger 3：请求来说明这一点第一个元素加载一个cache line，这个cache line也包含第二个元素。然而，第三个元素在下一个cache line上，因此加载它会引起主内存的延迟和带宽。第四个元素也是如此。加载四个元素现在需要加载三条缓存线而不是一条，这意味着三分之二的可用带宽被浪费了。(如果没有注意到常规访问模式的硬件机制，并先发制人地加载进一步的cache lines，第二种情况也会导致三倍于第一种情况的延迟;见部分1.3.5。)

<img src="/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/stride-3.jpg" alt="stride-3" style="zoom:50%;" />

有些应用程序自然会导致大于1的进步，例如，只访问一个复数数组的实数部分(关于复数实际实现的一些注释，请参阅3.7.6节)。另外，使用递归加倍的方法通常具有非单位步长的代码结构

```c
for (i=0; i<N/2; i++)
  x[i] = y[2*i];
```

在这个关于cache line的讨论中，我们隐式地假设cache line的开头也是一个单词的开头，不管是整数还是浮点数。这并不一定是真的：一个8字节的浮点数可以放置在两个cache line之间的边界上。您可以想象，这对性能是不利的。第37.1.3节讨论了实际中处理cache line边界对齐的方法。



#### 缓存映射

缓存变得更快，但也更小，它们越接近fpu，但即使是最大的缓存也比主内存小得多。在第1.3.4.6节中，我们已经讨论了如何做出保留哪些元素和替换哪些元素的决定。

现在我们将讨论缓存映射的问题，也就是“如果一个条目被放在缓存中，它会被放在哪里”的问题。这个问题通常是通过将项目的(主存)地址映射到缓存中的地址来解决的，这就导致了“如果两个项目映射到同一个地址会怎么样”的问题。



#### 直接映射缓存

最简单的缓存映射策略是直接映射。假设内存地址是32位长，因此它们可以寻址4G字节；进一步假设缓存有8K个字，也就是64K字节，需要16位来寻址。然后直接映射从每个内存地址取最后(“最低有效”)16位，并使用这些作为缓存中的数据项的地址;参见图1.8。

直接映射的效率非常高，因为它的地址计算速度非常快，导致了较低的延迟，但在实际应用中存在一个问题。如果两个被8K字分隔的条目被寻址，它们将被映射到相同的缓存位置，这将使某些计算效率低下。例子：

```c
double A[3][8192];
for (i=0; i<512; i++)
  a[2][i] = (a[0][i]+a[1][i])/2.;
```

![directmap](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/directmap.jpeg)

![directmapconflict](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/directmapconflict.jpeg)

在Fortran中：

```fortran
real*8 A(8192,3);
   do i=1,512
     a(i,3) = ( a(i,1)+a(i,2) )/2
   end do
```

此处，[0] [i]，[1] [i]和[2] [ i ] (或者a(i,1)，a(i,2)，a(i,3))的位置对于每个i来说是8K的，所以它们地址的最后16位是相同的，因此它们将被映射到缓存中的相同位置；参见图1.9。

现在，循环的执行情况将如下：

- a [0] [0]处的数据被带入高速缓存和寄存器。这就产生了一定的延时。和这个元素一起，整个高速缓存行被转移。
- 在[1] [0]处的数据被带入缓存（和寄存器，因为我们从现在开始不会再remark了），连同其整个缓存行，以一些延迟为代价。由于这个缓存行和第一个缓存行被映射到相同的位置，所以第一个缓存行被覆盖。
- 为了写入输出，包含a[2] [0]的缓存行被带入内存。这又被映射到同一位置，导致刚刚加载的a[1][0]的缓存行被刷新。
- 在下一次迭代中，需要a[0] [1]，它和a[0] [0]在同一个缓存行。然而，这个缓存行已经被刷新了，所以它需要从主内存或更深的缓存层中被重新带入。在这样做的时候，它覆盖了保存a[2] [0]的缓存行。
- a[1] [1]的情况类似：它在a[1] [0]的缓存行上，不幸的是，它已经被上一步覆盖了。

如果一个缓存行有四个字，我们可以看到，循环的每四次迭代都涉及到八个a元素的传输的元素，而如果不是因为缓存冲突，两个元素就足够了。

**练习 1.8 **在直接映射高速缓存的例子中，从内存到高速缓存的映射是通过使用32位内存地址的最后16位作为高速缓存地址完成的。如果使用前16位（"最有意义的"）作为缓冲区地址，那么这个例子中的问题就会消失。为什么在一般情况下这不是一个好的解决方案？

**注释5：**到目前为止，我们一直假装缓存是基于虚拟内存地址的。实际上，缓存是基于内存中数据的物理地址，这取决于将虚拟地址映射到内存页的算法。



#### 关联式缓存

如果任何数据项目都可以进入任何缓存位置，那么上一节中概述的缓存冲突问题就会得到解决。在这种情况下，除了缓存被填满之外，不会有任何冲突，在这种情况下，缓存替换策略（第1.3.4.6节）会刷新数据，为新来的项目腾出空间。这样的缓存被称为完全关联的，虽然它看起来是最好的，但它的构建成本很高，而且在使用中比直接映射的缓存慢得多。

由于这个原因，最常见的解决方案是建立一个$𝑘-way$关联的高速缓存，其中$𝑘$至少是两个。在这种情况下，一个数据项可以进入任何一个$𝑘$缓存位置。代码必须要有$𝑘+1$路冲突，才会像上面的例子那样过早地刷新数据。在这个例子中，$𝑘=2$的值就足够了，但在实践中经常会遇到更高的值。图1.10展示了一个直接映射的和一个三向关联的缓冲区的内存地址与缓冲区位置的映射情况。两个缓冲区都有12个元素，但是它们的使用方式不同。直接映射的高速缓存（左边）在内存地址0和12之间会有冲突，但是在三向关联高速缓存中，这两个地址可以被映射到三个元素中的任何一个。

![assoc-mapping](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/assoc-mapping.jpeg)

作为一个实际的例子，英特尔Woodcrest处理器有一个32K字节的L1缓存，它是8路设置的关联性，缓存行大小为64字节，L2缓存为4M字节，是8路设置的关联性，缓存行大小为64字节。另一方面，AMD Barcelona芯片的L1缓存是2路关联，L2缓存是8路关联。更高的关联性（'way-ness'）显然是可取的，但是会使处理器变得更慢，因为确定一个地址是否已经在缓存中变得更加复杂。由于这个原因，在速度最重要的地方，L1高速缓存的关联性通常比L2低。

**练习 1.9** 用你喜欢的语言写一个小型的高速缓存模拟器。假设一个有32个条目的𝑘方式的同构缓存和一个16位地址的架构。对$𝑘=1, 2, 4, ...$进行以下实验。

1. 让𝑘为模拟高速缓存的关联性。

2. 写下从16位内存地址到32/𝑘缓存地址的转换。

3. 生成32个随机机器地址，并模拟将其存储在缓存中。

   由于高速缓存有32个条目，最佳情况下这32个地址都可以存储在高速缓存中。这种情况实际发生的几率很小，往往一个地址的数据会在另一个地址与之冲突时被驱逐出缓存（意味着它被覆盖）。记录在模拟结束时，32个地址中，有多少地址被实际存储在缓存中。将步骤3做100次，并绘制结果；给出中位数和平均值，以及标准偏差。观察一下，增加关联性可以提高存储地址的数量。其极限行为是什么？(为了获得奖励，请做一个正式的统计分析）

   

#### 缓存内存与普通内存的对比

那么，缓冲存储器有什么特别之处；为什么我们不把它的技术用于所有的存储器？

缓存通常由静态随机存取存储器（SRAM）组成，它比用于主存储器的动态随机存取存储器（DRAM）更快，但也更昂贵，每一位需要5-6个晶体管，而不是一个，而且耗电量更大。



#### 负载与存储

在上述描述中，在程序中访问的所有数据都需要在使用这些数据的指令执行之前被移入高速缓存。这对读取的数据和写入的数据都适用。然而，已经写入的数据，如果不再需要（在一定的合理时间内），就没有理由留在缓存中，可能会产生冲突或驱逐仍然可以重复使用的数据。出于这个原因，编译器通常支持流式存储：一个纯粹输出的连续数据流将被直接写入内存，而不被缓存。



### 预取流

在传统的冯-诺依曼模型中（第1.1节），每条指令都包含其操作数的位置，所以实现这种模型的CPU会对每个新的操作数进行单独请求。在实践中，往往后续的数据项在内存中是相邻的或有规律的间隔。内存系统可以通过查看高速缓存的数据模式来检测这种数据模式，并请求预取数据流；

![prefetch](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/prefetch.jpeg)

在最简单的形式下，CPU会检测到连续的负载来自于两个连续的高速缓存行，并自动发出对接下来的高速缓存行的请求。如果代码对第三条高速缓存线发出实际请求，这个过程可以重复或扩展。由于这些高速缓存行现在是在需要之前就从内存中带来的，所以预取有可能消除除前几个数据项之外的所有延迟。

现在我们需要重新审视一下缓存缺失的概念。从性能的角度来看，我们只对缓存缺失的停顿感兴趣，也就是说，在这种情况下，计算必须等待数据被带入。不在缓存中的数据，但在其他指令还在处理的时候可以被带入，这不是一个问题。如果 "L1缺失 "被理解为只是 "缺失时的停顿"，那么术语 "L1缓存重新填充 "被用来描述所有的缓存线负载，无论处理器是否在它们上面停顿。

由于预取是由硬件控制的，所以它也被描述为硬件预取。预取流有时可以从软件中控制，例如通过本征。

由程序员引入预取是对一些因素的谨慎平衡[94]。其中最重要的是预取距离：从预取开始到需要数据时的周期数。在实践中，这通常是一个循环的迭代次数：预取指令请求未来迭代的数据。



### 并发和内存传输

在关于内存层次的讨论中，我们提出了一个观点：内存比处理器慢。如果这还不够糟糕的话，利用内存提供的所有带宽甚至不是小事。换句话说，如果你不仔细编程，你会得到比你根据可用带宽所期望的更少的性能。让我们来分析一下。

内存系统的带宽通常为每周期一个以上的浮点数，所以你需要每周期发出那么多请求来利用可用的带宽。即使在零延迟的情况下也是如此；由于存在延迟，数据从内存中出来并被处理需要一段时间。因此，任何基于第一个数据的计算而请求的数据都必须在延迟至少等于内存延迟的情况下请求。

为了充分利用带宽，在任何时候都必须有相当于带宽乘以延迟的数据量在飞行。的数据量必须在飞行中。由于这些数据必须是独立的，我们得到了利特尔定律的声明[147]。

$$
并发=带宽\times延迟
$$
![little](/Users/mac/Desktop/parall/高性能计算教材/第一部分：理论/graphics/little.jpg)

这在图1.12中得到了说明。维护这种并发性的问题并不是程序没有这种并发性，而是程序要让编译器和运行时系统识别它。例如，如果一个循环遍历了一个长的数组，编译器就不会发出大量的内存请求。预取机制（1.3.5节）会提前发出一些内存请求，但通常不够。因此，为了使用可用的带宽，多个数据流需要同时进行。因此，我们也可以将李特定律表述为
$$
有效吞吐量 = 表达的并发性/延迟
$$


### Memory banks

上面，我们讨论了与带宽有关的问题。你看到内存，以及在较小程度上的缓存，其带宽低于处理器可以最大限度地吸收的带宽。这种情况实际上比上面的讨论看起来还要糟糕。由于这个原因，内存通常被分为交错的内存组：在四个内存组中，字0、4、8...在0组，字1、5、9...在1组，依此类推。

假设我们现在按顺序访问内存，那么这样的4路交错式内存可以维持4倍于单个内存组的带宽。不幸的是，按跨度2访问将使带宽减半，而更大的跨度则更糟糕。如果两个连续的操作访问同一个内存库，我们就会说到内存库冲突[7]。在实践中，内存库的数量会更多，因此，小跨度的内存访问仍然会有完整的广告带宽。例如，Cray-1有16个库，而Cray-2有1024个。

**练习 1.10** 证明在有质数的库时，任何达到该质数的跨步都是无冲突的。你认为为什么这个解决方案没有在实际的内存架构中被采用？

在现代处理器中，DRAM仍然有bank，但由于有缓存的存在，其影响较小。然而，GPU有内存组，没有缓存，所以它们遭受了与老式超级计算机相同的一些问题。

**练习 1.11** 对一个数组的元素进行求和的递归加倍算法是。

```c
for (s=2; s<2*n; s*=2)
  for(i=0; i<n-s/2; i+=s)
    x[i] += x[i+s/2]
```

分析该算法的bank冲突。假设$𝑛=2^p$，bank有$2^k$元素，其中$𝑘 < 𝑝$。同时考虑到这是一个并行算法，内循环的所有迭代都是独立的，因此可以同时进行。另外，我们可以使用递归减半法。

```c
for (s=(n+1)/2; s>1; s/=2)
  for(i=0; i<n; i+=1)
    x[i] += x[i+s]
```

再次分析，bank的混乱情况。这种算法更好吗？在并行情况下呢？

缓存存储器也可以使用bank。例如，AMD巴塞罗那芯片的L1缓存中的缓存线是16个字，分为两个8个字的交错库。这意味着对高速缓存线的元素进行顺序访问是有效的，但串联访问的性能就会下降。



### TLB、页和虚拟内存

一个程序的所有数据可能不会同时出现在内存中。这种情况可能由于一些原因而发生。

- 计算机为多个用户服务，所以内存并不专门用于任何一个用户。
- 计算机正在运行多个程序，这些程序加起来需要的内存超过了物理上可用的内存。 
- 一个单一的程序所使用的数据可能超过可用的内存。

基于这个原因，计算机使用虚拟内存：如果需要的内存比可用的多，某些内存块会被写入光盘。实际上，光盘充当了真实内存的延伸。这意味着一个数据块可以出现在内存的任何地方，事实上，如果它被换入和换出，它可以在不同时间出现在不同位置。交换不是作用于单个内存位置，而是作用于内存页：连续的内存块，大小从几千字节到几兆字节。(在早期的操作系统中，将内存移至磁盘是程序员的责任。互相替换的内存页被称为覆盖层）

由于这个原因，我们需要一个从程序使用的内存地址到内存中实际地址的翻译机制，而且这种翻译必须是动态的。一个程序有一个 "逻辑数据空间"（通常从地址0开始），是编译后的代码中使用的地址，在程序执行过程中需要将其翻译成实际的内存地址。出于这个原因，有一个页表，指定哪些内存页包含哪些逻辑页。

#### Large pages

在非常不规则的应用中，例如数据库，页表会变得非常大，因为更多或更少的随机数据被带入内存。然而，有时这些页面显示出某种程度的集群，这意味着如果页面大小更大，需要的页面数量将大大减少。由于这个原因，操作系统可以支持大的页面，通常大小为2Mb左右。(有时会使用'巨大的页面'；例如，英特尔Knights Landing有Gigabyte页面）

大页面的好处取决于应用：如果小页面没有足够的集群，使用大页面可能会使内存过早地被大页面的未使用部分填满。



#### TLB

然而，通过查找该表进行地址转换是很慢的，所以CPU有一个翻译查找缓冲区（TLB）。TLB是一个经常使用的页表项的缓存：它为一些页提供快速的地址转换。如果一个程序需要一个内存位置，就会查询TLB，看这个位置是否真的在TLB所记忆的页面上。如果是这样，逻辑地址就被翻译成物理地址；这是一个非常快速的过程。在TLB中没有记住该页的情况被称为TLB缺失，然后查询页面查找表，如果有必要，将需要的页面带入内存。TLB是（有时是完全）关联的（1.3.4.10节），使用LRU策略（1.3.4.6节）。

一个典型的TLB有64到512个条目。如果一个程序按顺序访问数据，它通常只在几个页面之间交替进行，而且不会出现TLB缺失。另一方面，一个访问许多随机内存位置的程序可能会因为这种错过而出现速度下降。目前正在使用的页面集被称为 "工作集"。

第1.7.5节和附录37.5讨论了一些简单的代码来说明TLB的行为。

[这个故事有一些复杂的情况。例如，通常有一个以上的TLB。第一个与L2缓存相关，第二个与L1相关。在AMD Opteron中，L1 TLB有48个条目，并且是完全（48路）关联的，而L2 TLB有512个条目，但只是4路关联的。这意味着实际上可能存在TLB冲突。在上面的讨论中，我们只谈到了L2 TLB。之所以能与L2缓存而不是主内存相关联，是因为从内存到L2缓存的转换是确定性的]。

使用大页也可以减少潜在的TLB缺失次数，因为可以减少工作页的集合。
